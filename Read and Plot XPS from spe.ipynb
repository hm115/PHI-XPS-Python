{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Updated-tgmeow_Read and Plot Spectrum Data.ipynb","provenance":[{"file_id":"1-S9JK9aqv0B8-4G2iEovSZROuiCqWdU8","timestamp":1616527773421},{"file_id":"1RsPw2ZW0Z3LYVccJtxjC3FfbDIODYQ40","timestamp":1616526980863}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.8.5 64-bit ('datsci': conda)"},"language_info":{"name":"python","version":"3.8.5"},"interpreter":{"hash":"5a609d29712f46cb739538e77019f71abfad367d089f9415cf30babbf69751d1"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jWsQzLwbwOFS","executionInfo":{"status":"ok","timestamp":1623175921351,"user_tz":240,"elapsed":205,"user":{"displayName":"Hansen Mou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpX0kAdNL6lFvVSEqTwcLfrTvr1l-z768ZLhGZ=s64","userId":"05725151348395746586"}},"outputId":"2cbf1fee-7f09-49ae-c3be-de4e7306c78c"},"source":["# from google.colab import drive\r\n","\r\n","# drive.mount('/content/drive')"],"execution_count":1,"outputs":[]},{"source":["# Run the following 2 blocks. \n","## Then, start where it says Step 1/4. Run all blocks unless specified otherwise.\n","\n","Make sure lmfit, ipympl, and ipywidgets are installed in your python environment!"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"5UECFNr8mttl","executionInfo":{"status":"ok","timestamp":1623175922772,"user_tz":240,"elapsed":206,"user":{"displayName":"Hansen Mou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpX0kAdNL6lFvVSEqTwcLfrTvr1l-z768ZLhGZ=s64","userId":"05725151348395746586"}}},"source":["import numpy as np\n","import struct\n","import array\n","\n","import ipywidgets as widgets\n","import matplotlib.pyplot as plt\n","import sys\n","import re\n","from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n","\n","from lmfit import Model\n","\n","import sys\n","import tkinter\n","from tkinter.filedialog import askopenfilenames"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M2ztiBIu4hTH"},"source":["The code block below is modified by Tiger Mou from the Python3convert-multipak-to-XPSPEAK-columbia.py file that was originally written by Robert Forest in Python 2, modified for Python 3 by Hansen Mou. Plotting and fitting was written by Hansen Mou."]},{"cell_type":"code","metadata":{"id":"OSSWTZLalUVz","executionInfo":{"status":"ok","timestamp":1623175925642,"user_tz":240,"elapsed":416,"user":{"displayName":"Hansen Mou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpX0kAdNL6lFvVSEqTwcLfrTvr1l-z768ZLhGZ=s64","userId":"05725151348395746586"}}},"source":["### This block imports the spe file.\n","\n","class XSpectrum:\n","\n","    def __init__(self):\n","        self._regions = []\n","        self._XPS_REGION_START_STR = b'DP'\n","        self._XPS_HEADER_SIZE = 42\n","   \n","    def load_xps(self, path):\n","        \"\"\"\n","        Read and parse XPS file according to the reverse engineering of\n","        Robert Forest's Python3convert-multipak-to-XPSPEAK-columbia.py\n","        \"\"\"\n","        file_str = \"\"\n","        # We read in the entire file because iterating through is tedious.\n","        with open(path, 'rb') as f:\n","            print('Reading XPS from ' + path)\n","            file_str = f.read();\n","            f.close()\n","\n","        version_str = file_str[0:11].decode('utf-8')\n","        print(f'File version is: {version_str}')\n","\n","        idx = file_str.find(self._XPS_REGION_START_STR) + len(self._XPS_REGION_START_STR)\n","        while idx != -1:\n","            print(f'IDX: {idx}')\n","            (region, size) = self.__xps_read_region(file_str, idx)\n","            self._regions.append(region)\n","            idx = file_str.find(self._XPS_REGION_START_STR, idx + size) + len(self._XPS_REGION_START_STR)\n","\n","        print('Done!')\n","    \n","  \n","    def load_spe(self, path):\n","        \"\"\"\n","        Read and parse SPE file according to the reverse engineering of\n","        Robert Forest's Python3convert-multipak-to-XPSPEAK-columbia.py\n","        \"\"\"\n","        with open(path, 'rb') as f:\n","            print('Reading SPE from ' + path)\n","            num_regions = self.__spe_get_number_of_regions(f)\n","            BE_parameters = self.__spe_get_BE_parameters(f, num_regions)\n","            self.__spe_convert_multipak_file(f, BE_parameters, num_regions)\n","        print('Done!')\n","\n","    # XPS helper functions\n","    def __xps_read_region(self, file_str, idx):\n","        \"\"\"\n","        \"\"\"\n","        curr_reg = dict()\n","\n","        \"\"\" Header contains: (total 42 bytes)\n","        0: region_size (2 bytes)\n","        1: unknown: uint 655353 (4)\n","        2: 20 characters for the region_name, with whitespace padding (20)\n","        3: region_size again ?? (2)\n","        4: unknown: int of 1 (2)\n","        5: region_size + 1 (2)\n","        and 10 zeros to end the header (10)\n","        \"\"\"\n","        region_header = struct.unpack_from('<HI20sHHH10x', file_str, idx)\n","        print(region_header)\n","        idx += self._XPS_HEADER_SIZE\n","\n","        curr_reg['region_size'] = region_header[0]\n","        curr_reg['region_name'] = region_header[2].strip()\n","        \n","        # Read region be\n","        curr_reg['be_data'] = np.array(struct.unpack_from(f'<{region_header[0]}f', file_str, idx))\n","        idx += region_header[0] * 4\n","\n","        # Strange whitespace of 1, region_size+1, and 10 zeros\n","        unknown_whitespace = struct.unpack_from('<HH10x', file_str, idx)\n","        idx += 14\n","\n","        # Read region intensity\n","        curr_reg['intensity'] = np.array(struct.unpack_from(f'<{region_header[0]}f', file_str, idx))\n","        idx += region_header[0] * 4\n","\n","        \"\"\" Footer contains:\n","        0: max_BE\n","        1: min_BE\n","        2: max_intensity\n","        3: min_intensity\n","        and LOTS of whitespace for some reason...\n","        \"\"\"\n","        region_footer = struct.unpack_from('<4f', file_str, idx)\n","        idx += 76\n","\n","        curr_reg['max_be'] = region_footer[0]\n","        curr_reg['min_be'] = region_footer[1]\n","        curr_reg['max_intensity'] = region_footer[2]\n","        curr_reg['min_intensity'] = region_footer[3]\n","\n","        # I think it's nothing but idk why\n","        region_post_footers = []\n","        for i in range(6):\n","            region_post_footers.append(struct.unpack_from(f'<HH10x{region_header[0]}f', file_str, idx + i*18+region_header[0]*4))\n","\n","        curr_reg['post_footers'] = region_post_footers\n","        \n","        region_size = self._XPS_HEADER_SIZE + region_header[0] * 4 * 2 + 14 + 16 + (6 *(2+2+region_header[0]*4+10)) + 18\n","\n","        return (curr_reg, region_size)\n","\n","\n","    # SPE helper functions\n","\n","    def __spe_get_number_of_regions(self, file):\n","        \"\"\"\n","        Returns the total number of elements (\"regions\") from multi-scan\n","        \"\"\"\n","        file_string = file.read()\n","        i = file_string.rfind(b'NoSpectralReg: ') + len('NoSpectralReg: ')\n","        file.seek(i)\n","        return int(file.readline())\n","\n","    def __spe_get_BE_parameters(self, file, num_regions):\n","        \"\"\"Returns a 2D list of binding energy parameters as strings\n","        First dimension is the region number\n","        Second dimension is a list of individual parameters for that region\n","        Indices: 3-region name, 5-region size, 6-step size, 7-high BE, 8-low BE\n","        \"\"\"\n","        BE_parameters = []\n","        for line_number in range(num_regions):\n","            BE_parameters.append(file.readline())\n","            BE_parameters[line_number] = BE_parameters[line_number].split(b' ')\n","        return BE_parameters\n","\n","    def __spe_convert_multipak_file(self, file, BE_parameters, num_regions):\n","        # Set up regions.\n","        self._regions = [dict() for _ in range(num_regions)]\n","\n","        # Go to start of data in multipak file.\n","        data_size = 0\n","        for i in range(num_regions):\n","            data_size += int(BE_parameters[i][5])\n","        file.seek(-8 * data_size, 2) \n","\n","        # Process data in each region.\n","        for i in range(num_regions):\n","            # Reference to regions dict.\n","            curr_reg = self._regions[i]\n","\n","            region_params = BE_parameters[i]\n","            curr_reg['max_be'] = float(region_params[7])\n","            curr_reg['min_be'] = float(region_params[8])\n","            curr_reg['region_size'] = int(region_params[5])\n","            intensity_data_double = array.array('d')\n","            intensity_data_double.fromfile(file, curr_reg['region_size'])\n","                        \n","            # Region metadata.  Not sure if useful.\n","            curr_reg['region_name'] = region_params[3]\n","\n","            # I guess XPS format doesn't support double?\n","            curr_reg['be_data'] = np.linspace(curr_reg['max_be'], curr_reg['min_be'], curr_reg['region_size'])\n","\n","            curr_reg['intensity'] = np.array(intensity_data_double)\n","\n","            curr_reg['min_intensity'] = min(curr_reg['intensity'])\n","            curr_reg['max_intensity'] = max(curr_reg['intensity'])\n","\n","def get_file_names():    \n","    root = tkinter.Tk()\n","    root.withdraw()\n","    root.attributes('-topmost',True)\n","    file_list = askopenfilenames(parent=root, title='Open XPS files',\n","                                              filetypes=[('.spe', '*.spe')])\n","    file_list = root.tk.splitlist(file_list) #workaround for Windows bug\n","    root.destroy()\n","    return file_list\n","\n","### Shirley background algorithm\n","\n","### Brief explanation of the algorithm: Shirley background is generated to be S(E) = y_min + interval * { A(2) / [A(1)+A(2)] }\n","##  Where S(E) is the background intensity at a given binding energy E\n","##  A(2) is the area at binding energies less than E, and A(1) is area at binding energies greater than E\n","##  interval is usually accepted to be the intensity difference of the lower and upper bounds for binding energy, for our background region of interest\n","##  This implementation first calculates the total area [A(1) + A(2)] by numerically integrating via the trapezoid method to find k_integral, and then k_val is simply \" interval / [A(1) + A(2)]\"\n","##  Next we calculate A(2) (designated y_integral) and multiply it with k_val\n","##  Once the iteration has completed, we add y_min to the background\n","##  Tolerance is determined by calculating the norm of the difference of the initial and final background arrays. I don't understand how that works, but norm basically indicates the degree of difference between 2 arrays, so the smaller the norm, the higher tolerance. Unit is arbitrary, I think.\n","def shirley_func(ind_min,ind_max,be_vals):\n","    shirley_back = np.zeros(be_vals.shape)      ## initialize shirley_background array, same size as the entire domain of the region\n","    for i in range(ind_min,ind_max+1):          ## populate shirley_back array with values for linear background\n","        shirley_back[i] = ((y_max - y_min)/(x_max - x_min)) * (be_vals[i] - x_min)\n","    shirley_back_it = shirley_back.copy()       ## copy shirley_back array for a separate, iterated/manipulated version\n","\n","    tol = 0.01      ## Tolerance level\n","\n","    it_max = 10       ## maximum number of iterations\n","    it = 0              ## initial iteration value\n","\n","    while it < it_max:\n","        k_integral = 0.0\n","        for i in range(ind_max, ind_min):\n","            k_integral += (be_vals[i+1] - be_vals[i]) * 0.5 * (intensity_vals[i+1] + intensity_vals[i] - 2 * y_min - shirley_back[i+1] - shirley_back[i]) # integration via trapezoid method\n","        k_val = (y_max - y_min)/k_integral\n","            \n","        for i in range(ind_max, ind_min):\n","            y_integral = 0.0\n","            for j in range(i, ind_min):\n","                y_integral += (be_vals[j+1] - be_vals[j]) * 0.5 * (intensity_vals[j+1] + intensity_vals[j] - 2 * y_min - shirley_back[j+1] - shirley_back[j])\n","            shirley_back_it[i] = k_val * y_integral\n","        \n","        if np.linalg.norm(shirley_back_it - shirley_back) < tol:\n","            shirley_back = shirley_back_it.copy()\n","            break\n","        else:\n","            shirley_back = shirley_back_it.copy()\n","        it +=1\n","\n","    if it >= it_max:\n","        print(\"max iterations exceeded\")\n","\n","    shirley_back_fin = y_min + shirley_back\n","\n","    print(\"Number of iterations: \" + str(it))\n","    return shirley_back_fin\n","    # print(shirley_back_fin)\n","\n","    # print(shirley_back_fin)e(figsize=(10,5))\n","\n","### Gaussian\n","def gauss(x,E,F,area,m):\n","    # E = pars['E']\n","    # F = pars['F']\n","    # # a = param[2]\n","    # m = pars['m']\n","    # model = height* np.exp(-4 * np.log(2) * (1 - m/100) * ((x - E) / F)**2)\n","    model = (2 * np.sqrt(np.log(2))/(F * np.sqrt(np.pi))) * area * np.exp(-4 * np.log(2) * (1 - m/100) * ((x - E) / F)**2)\n","    return model\n","\n","# Note: for Gaussian, FWHM = 2 * sigma * sqrt(2 ln(2))\n","\n","### Lorentzian\n","def lorentz(x,E,F,area,m):\n","    # E = pars['E']\n","    # F = pars['F']\n","    # # a = param[2]\n","    # m = pars['m']\n","    # model = height / ((1 + 4 * m/100 * ((x - E) / F)**2))\n","    model = 2 * area / (np.pi * (1 + 4 * m/100 * ((x - E) / F)**2) * F)\n","    return model\n","# !!!! I don't understand why this equation shouldn't be multiplied by 2. !!!!\n","\n","# Note: for Lorentzian, FWHM = 2 sigma\n","\n","### E = position (eV); F = FWHM; m = % Lorentzian\n","\n","### Sum G/L\n","def sum_gl(x,E,F,area,m):\n","    # E = param[0]\n","    # F = param[1]\n","    # a = param[2]\n","    # m = param[3]\n","    model = ((1-m/100) * gauss(x,E,F,area,0) + (m/100) * lorentz(x,E,F,area,100))\n","    return model\n","\n","### Product G/L\n","## Has issues with returning the correct area. I think it has to do with how the weight factor 'm' is incorporated. May need to do numerical integration.\n","def product_gl(x,E,F,area,m):\n","    # E = pars['E']\n","    # F = pars['F']\n","    # # a = param[2]\n","    # m = pars['m']\n","    model = (gauss(x,E,F,area,m) * lorentz(x,E,F,area,m))\n","    return model\n","\n","### Defining default variable valus\n","class xps_settings():\n","    def __init__(self, region=None, peak_type=\"singlet\", background_lower_bound=None, background_upper_bound=None, num_peaks=1, param=None, peak_id=['p1_','p1_2_','p2_','p2_2_'], coup_E=None,coup_area=None, shift=0):\n","        self.region = region\n","        self.peak_type = peak_type\n","        self.background_lower_bound = background_lower_bound\n","        self.background_upper_bound = background_upper_bound\n","        self.num_peaks = num_peaks\n","        self.param = param\n","        self.peak_id = peak_id\n","        self.coup_E = coup_E\n","        self.coup_area = coup_area\n","        self.shift = shift\n","\n","# peak_type = \"couplets\"   # singlet or couplets\n","# background_lower_bound = 24.8\n","# # background_upper_bound = 37.25\n","# num_peaks = 4\n","# param = [[28,1.0,100,80],[30,1.0,100,80],[30,1.0,100,80],[33,1.0,100,80]]\n","# peak_id = ['p1_','p1_2_','p2_','p2_2_']\n","# coup_E = 1.91\n","# coup_area = (3/4)\n","\n","# shift = -5.5\n","\n","# fwhm_min = 0\n","# fwhm_max = 1.7\n","# area_min = 0.1\n","# gl_min = 0\n","# gl_max = 100"],"execution_count":5,"outputs":[]},{"source":["# Step 1/4: Start here! Run the block below to import your SPE file and get a plot of the raw data.\n","You can mouse over the plot to see the coordinates!"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"2Q1WtsQzm6SM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623176511481,"user_tz":240,"elapsed":199,"user":{"displayName":"Hansen Mou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpX0kAdNL6lFvVSEqTwcLfrTvr1l-z768ZLhGZ=s64","userId":"05725151348395746586"}},"outputId":"21c776a3-ed0b-4d52-fdc4-7e70c2a1d3e8"},"source":["### Import file\n","file = get_file_names()\n","\n","data1 = XSpectrum()\n","\n","if len(file) > 0:\n","    data1.load_spe(file[0])\n","\n","### Identify the regions in the imported file\n","reg_len = len(data1._regions)\n","label = []\n","for i in range(reg_len):\n","    label.append(data1._regions[i]['region_name'].decode('UTF-8'))\n","print(\"\\nRegions identified: \" + str(label))\n","\n","### Assigning names to each set of data\n","id_data1 = {}\n","for i in range(reg_len):\n","    id_data1[label[i]] = data1._regions[i]"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading SPE from G:/My Drive/Forschung/Data/XPS/XPS Data/TaN/210218-TaN(comm)-fine0002.SPE\nDone!\n\nRegions identified: ['C1s', 'Ta4f', 'N1s', 'O1s', 'In3d5']\n"]}]},{"source":["## Run below to see plots of your raw data"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"output_type":"display_data","data":{"text/plain":"Tab(children=(Output(), Output(), Output(), Output(), Output()), _titles={'0': 'C1s', '1': 'Ta4f', '2': 'N1s',…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a1a07352b2b4d37886d9840a1658d54"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["9"]},"metadata":{},"execution_count":7}],"source":["### Plot the raw data\n","%matplotlib widget\n","sub_tab=[widgets.Output() for i in range(reg_len)]\n","tab = widgets.Tab(sub_tab)\n","for i in range(reg_len):\n","    tab.set_title(i,label[i].format(i+1))\n","    with sub_tab[i]:\n","        fig = plt.figure(figsize=(10,5))\n","        ax = fig.subplots()\n","        ax.plot(data1._regions[i]['be_data'], data1._regions[i]['intensity'])\n","        ax.set_title(label[i],fontweight='bold',fontsize=20)\n","        ax.invert_xaxis()\n","        ax.set_xlabel('Binding Energy [eV]',fontweight='bold',fontsize=16)\n","        ax.set_ylabel('Intensity',fontweight='bold',fontsize=16)\n","        ax.tick_params(axis='both',which='major',labelsize=14)\n","        ax.xaxis.set_minor_locator(MultipleLocator(100))\n","        plt.show(fig)\n","display(tab)\n","def mouse_move(event):\n","    x, y = event.xdata, event.ydata\n","    print(x, y)\n","plt.connect('motion_notify_event', mouse_move)\n","# plt.show()"]},{"source":["Note: if couplets, 2nd init param binding energy should be autocalculated. also auto-implement area coupling and maybe binding energy coupling?"],"cell_type":"markdown","metadata":{}},{"source":["## Step 2/4: Set your peak fitting configuration here"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["### C1s\n","# peak_input = xps_settings(\n","#     region = \"C1s\",\n","#     peak_type = \"singlet\",   # singlet or couplets\n","#     background_lower_bound = 281.3,\n","#     background_upper_bound = 295.85,\n","#     num_peaks = 2,\n","#     param = [[288.1,1.0,100,80],[290,1.0,100,80]],\n","#     peak_id = ['p1_','p2_'],\n","#     # shift = -4.5\n","#     )\n","\n","### O1s\n","# peak_input = xps_settings(\n","#     region = \"O1s\",\n","#     peak_type = \"singlet\",   # singlet or couplets\n","#     background_lower_bound = 530.2,\n","#     background_upper_bound = 541.5,\n","#     num_peaks = 2,\n","#     param = [[535.45,1.0,100,80],[537.6,1.0,100,80]],\n","#     peak_id = ['p1_','p2_'],\n","#     # shift = -4.5\n","#     )\n","\n","### Pt4f7 \n","# peak_input = xps_settings(\n","#     region = \"Pt4f7\",\n","#     peak_type = \"couplets\",   # singlet or couplets\n","#     background_lower_bound = 69.65,\n","#     background_upper_bound = 86.95,\n","#     num_peaks = 2,\n","#     param = [[74.6,1.0,100,80],[78,1.0,100,80],[76,1.0,100,80],[79,1.0,100,80]],\n","#     peak_id = ['p1_','p1_2_','p2_','p2_2_'],\n","#     coup_E = 3.33,\n","#     coup_area = (3/4),\n","#     # shift = -4.7\n","#     )\n","\n","### Pd3d\n","# peak_input = xps_settings(\n","#     region = \"Pd3d\",\n","#     peak_type = \"couplets\",   # singlet or couplets\n","#     background_lower_bound = 335.1,\n","#     background_upper_bound = 349.95,\n","#     num_peaks = 4,\n","#     param = [[341,1.0,100,80],[341.2,1.0,100,80],[341.9,1.0,100,80],[346.2,1.0,100,80]],\n","#     peak_id = ['p1_','p1_2_','p2_','p2_2_'],\n","#     coup_E = 5.26,\n","#     coup_area = (2/3),\n","#     shift = -5.5,\n","#     )\n","\n","###  Nb3d\n","# peak_input = xps_settings(\n","#     region = \"Nb3d\",\n","#     peak_type = \"couplets\",   # singlet or couplets\n","#     background_lower_bound = 205,\n","#     # background_upper_bound = 349.95,\n","#     num_peaks = 4,\n","#     param = [[207.9,1.0,100,80],[211.4,1.0,100,80],[212,1.0,100,80],[214.7,1.0,100,80]],\n","#     peak_id = ['p1_','p1_2_','p2_','p2_2_'],\n","#     coup_E = 2.72,\n","#     coup_area = (2/3),\n","#     # shift = -4.5,\n","#     )\n","\n","# ### Ta4f\n","peak_input = xps_settings(\n","    region = \"Ta4f\",\n","    peak_type = \"couplets\",   # singlet or couplets\n","    background_lower_bound = 23.6,\n","    background_upper_bound = 36.5,\n","    num_peaks = 6,\n","    param = [[29,1.0,100,80],[27,1.0,100,80],[31,1.0,100,80]],\n","    peak_id = ['p1_','p1_2_','p2_','p2_2_','p3_','p3_2_'],\n","    coup_E = 1.91,\n","    coup_area = (3/4),\n",")\n","\n","\n","# shift = -4.62\n","\n","\n","# fwhm_min = 0\n","# fwhm_max = 1.7\n","# area_min = 0.1\n","# gl_min = 0\n","# gl_max = 100\n","\n","# peak_input = xps_settings(\n","#     region = \"C1s\",\n","#     # peak_type = None,\n","#     # background_lower_bound = None,\n","#     # background_upper_bound = None,\n","#     num_peaks = None,\n","#     param = None,\n","#     peak_id = None,\n","#     coup_E = None,\n","#     coup_area = None,\n","#     shift = None\n","#     )"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["[30.91, 1.0, 75.0, 80]\n"]}],"source":["coupled_param = [peak_input.param[0][0] + peak_input.coup_E, peak_input.param[0][1], peak_input.param[0][2] * peak_input.coup_area, peak_input.param[0][3]]\n","print(coupled_param)"]},{"source":["Start running the blocks below once satisfied with the configuration."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q63bMGS_MOtK","executionInfo":{"status":"ok","timestamp":1617823691188,"user_tz":240,"elapsed":30521,"user":{"displayName":"Hansen Mou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpX0kAdNL6lFvVSEqTwcLfrTvr1l-z768ZLhGZ=s64","userId":"05725151348395746586"}},"outputId":"26ef5fc9-1781-4004-e57b-9c934d7d13c9"},"source":["if peak_input.region is None:\n","    which_reg = \"Pt4f7\"\n","else:\n","    which_reg = peak_input.region\n","\n","if peak_input.region not in id_data1:\n","    print(\"The region you chose is not present. Please choose from the following:\")\n","    for i in id_data1:\n","        print(\"\\t\"+str(i))\n","else: \n","    be_vals = id_data1[peak_input.region]['be_data']\n","    activ_reg = label.index(peak_input.region)\n","    intensity_vals = id_data1[peak_input.region]['intensity']\n","    print(str(peak_input.region) + \" loaded!\")\n","    ### Setting background\n","    if peak_input.background_lower_bound is None:\n","        peak_input.background_lower_bound = be_vals[-1]\n","    if peak_input.background_upper_bound is None:\n","        peak_input.background_upper_bound = be_vals[0]\n","\n","    x_min = be_vals[np.abs(be_vals - peak_input.background_lower_bound).argmin()]\n","    x_max = be_vals[np.abs(be_vals - peak_input.background_upper_bound).argmin()]\n","    if x_min > x_max:\n","        print(\"Please switch your upper and lower bounds.\")\n","    elif x_max not in be_vals or x_min not in be_vals:\n","        print(\"Your selected bounds exceed the region. Please set bounds between \" + str(be_vals[-1]) + \" and \" + str(be_vals[0]))\n","    else:\n","        ind_min = np.where(be_vals == x_min)[0][0]\n","        ind_max = np.where(be_vals == x_max)[0][0]\n","\n","        y_min = id_data1[which_reg]['intensity'][ind_min]\n","        y_max = id_data1[which_reg]['intensity'][ind_max]\n","\n","        print(\"Done!\")\n","\n","    shirley_back_fin = shirley_func(ind_min, ind_max, be_vals)\n","\n","    ### Plotting initial parameters\n","    ### param = [peak binding energy, FWHM, area, % gauss/lorentz]\n","\n","    if peak_input.num_peaks is None:\n","        num_peaks = 2\n","\n","    if peak_input.param is None:\n","        param = [[74.6,1.0,100,80],[78,1.0,100,80],[211,1.0,100,80],[84,1.0,100,80]]\n","\n","    ### if fitting couplets, only need half as many parameters as peaks\n","\n","    ### Normalizing and zeroing the raw data for fitting\n","    i = activ_reg\n","\n","    intensity_zeroed = np.zeros(be_vals.shape)\n","\n","    for j in range(len(be_vals)):\n","        intensity_zeroed[j] = intensity_vals[j] - shirley_back_fin[j]\n","\n","    norm_factor = np.max(intensity_zeroed)\n","\n","    intensity_norm = intensity_zeroed / norm_factor    \n","\n","    ### Plots the initial guess on the figure\n","    initial_fit = []\n","    if peak_input.peak_type == \"couplets\":\n","        for i in range(peak_input.num_peaks):\n","            if i % 2 == 0:\n","                initial_fit.append(sum_gl(be_vals[ind_max:ind_min],*peak_input.param[int(i/2)]) + shirley_back_fin[ind_max:ind_min])\n","            else:\n","                coupled_param = [peak_input.param[int((i-1)/2)][0] + peak_input.coup_E, peak_input.param[int((i-1)/2)][1], peak_input.param[int((i-1)/2)][2] * peak_input.coup_area, peak_input.param[int((i-1)/2)][3]]\n","                initial_fit.append(sum_gl(be_vals[ind_max:ind_min],*coupled_param) + shirley_back_fin[ind_max:ind_min])\n","    elif peak_input.peak_type == \"singlet\":\n","        for i in range(peak_input.num_peaks):\n","            initial_fit.append(sum_gl(be_vals[ind_max:ind_min],*peak_input.param[i]) + shirley_back_fin[ind_max:ind_min])\n","\n","    initial_fit = np.array(initial_fit)\n","\n","    # make sure that you multiply the shirley_back_fin by (total number of initial fits minus 1!)\n","    initial_fit_sum = 0\n","    initial_fit_sum = np.sum(initial_fit,axis=0) - (peak_input.num_peaks-1) * shirley_back_fin[ind_max:ind_min]\n","\n","    fig = plt.figure(figsize=(10,5))\n","\n","    ax = fig.add_subplot(111)\n","\n","    ax.plot(be_vals, intensity_vals)       ## Plot Region in question\n","    ax.plot(be_vals[ind_max:ind_min],shirley_back_fin[ind_max:ind_min])    ## Plot Shirley background\n","    ax.plot(be_vals[ind_max:ind_min],initial_fit_sum)        ## Initial Fit\n","    for i in range(len(initial_fit)):\n","        ax.plot(be_vals[ind_max:ind_min],initial_fit[i])        ## Initial Fit\n","\n","    ax.set_title(label[activ_reg],fontweight='bold',fontsize=20)\n","    ax.invert_xaxis()\n","    ax.set_xlabel('Binding Energy [eV]',fontweight='bold',fontsize=16)\n","    ax.set_ylabel('Intensity',fontweight='bold',fontsize=16)\n","    ax.tick_params(axis='both',which='major',labelsize=14)\n","    fig.subplots_adjust(hspace = 0.4)\n","\n","    # plt.legend(['data', 'background', 'initial guess', 'final fit'])\n","    plt.legend(['data', 'background', 'overall fit'])\n","\n","    ax.xaxis.set_minor_locator(MultipleLocator(100))\n","\n","    # ax[0].set_ylim(400,480)\n","    # ax[1].set_ylim(0,500)\n","\n","    def mouse_move(event):\n","        x, y = event.xdata, event.ydata\n","        print(x, y)\n","\n","    plt.connect('motion_notify_event', mouse_move)\n","\n","    plt.show()"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Ta4f loaded!\n","Done!\n","Number of iterations: 4\n"]},{"output_type":"display_data","data":{"text/plain":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6dfab8b178a423cbec87b0b441d6950"}},"metadata":{}}]},{"source":["## If initial plot looks ok, then continue below.\n","Note: need to rewrite this to become more flexible, and clean up the parameters output"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["peak_param_id = ['E','F','area','m']\n","\n","fwhm_min = 0\n","fwhm_max = 1.7\n","area_min = 0.1\n","gl_min = 0\n","gl_max = 100\n","\n","peak_model_list = []\n","peak_model_list.append(Model(sum_gl,prefix=peak_input.peak_id[0]))\n","pars = peak_model_list[0].make_params()\n","\n","pars[peak_input.peak_id[0]+peak_param_id[0]].set(value=peak_input.param[0][0])\n","pars[peak_input.peak_id[0]+peak_param_id[1]].set(value=peak_input.param[0][1],min=0,max=1.7)\n","pars[peak_input.peak_id[0]+peak_param_id[2]].set(value=peak_input.param[0][2],min=0.1)\n","pars[peak_input.peak_id[0]+peak_param_id[3]].set(value=peak_input.param[0][3],min=0,max=100)\n","\n","if peak_input.peak_type == \"singlet\":\n","    for i in range(1,peak_input.num_peaks):\n","        peak_model_list.append(Model(sum_gl,prefix=peak_input.peak_id[i]))\n","        pars.update(peak_model_list[i].make_params())\n","\n","        pars[peak_input.peak_id[i]+peak_param_id[0]].set(value=peak_input.param[i][0])\n","        pars[peak_input.peak_id[i]+peak_param_id[1]].set(value=peak_input.param[i][1],min=0,max=1.7)\n","        pars[peak_input.peak_id[i]+peak_param_id[2]].set(value=peak_input.param[i][2],min=0.1)\n","        pars[peak_input.peak_id[i]+peak_param_id[3]].set(value=peak_input.param[i][3],min=0,max=100)\n","elif peak_input.peak_type == \"couplets\":\n","    for i in range(1,peak_input.num_peaks):\n","        peak_model_list.append(Model(sum_gl,prefix=peak_input.peak_id[i]))\n","        pars.update(peak_model_list[i].make_params())\n","        if i % 2 == 0:\n","            pars[peak_input.peak_id[i]+peak_param_id[0]].set(value=peak_input.param[int(i/2)][0])\n","            pars[peak_input.peak_id[i]+peak_param_id[1]].set(value=peak_input.param[int(i/2)][1],min=0,max=1.7)\n","            pars[peak_input.peak_id[i]+peak_param_id[2]].set(value=peak_input.param[int(i/2)][2],min=0.1)\n","            pars[peak_input.peak_id[i]+peak_param_id[3]].set(value=peak_input.param[int(i/2)][3],min=0,max=100)\n","        else:\n","            ### Set how the parameters for peak2_2 are bound by peak 2\n","\n","            # peak binding energy\n","            pars[peak_input.peak_id[i]+peak_param_id[0]].set(expr=peak_input.peak_id[i-1]+peak_param_id[0]+ ' + ' + str(peak_input.coup_E))\n","\n","            # peak FWHM\n","            pars[peak_input.peak_id[i]+peak_param_id[1]].set(expr=peak_input.peak_id[i-1]+peak_param_id[1])\n","\n","            # peak area (could probably automate this by extracting the orbital from the region name)\n","            pars[peak_input.peak_id[i]+peak_param_id[2]].set(expr=peak_input.peak_id[i-1]+peak_param_id[2] + ' * ' + str(peak_input.coup_area))\n","\n","            # % GL\n","            pars[peak_input.peak_id[i]+peak_param_id[3]].set(expr=peak_input.peak_id[i-1]+peak_param_id[3])\n","                \n","# print(pars)\n","# print(peak_model_list)\n","peaks_all = np.array(peak_model_list)\n","model = np.sum(peaks_all,axis=0)\n"]},{"source":["## Step 3/4: Run this block to perform the fit.\n","### Check the \"[[Variables]]\" block for the fitted parameters\n","\n","Need to format the output, include the shift"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["[[Model]]\n    (((((Model(sum_gl, prefix='p1_') + Model(sum_gl, prefix='p1_2_')) + Model(sum_gl, prefix='p2_')) + Model(sum_gl, prefix='p2_2_')) + Model(sum_gl, prefix='p3_')) + Model(sum_gl, prefix='p3_2_'))\n[[Fit Statistics]]\n    # fitting method   = least_squares\n    # function evals   = 17\n    # data points      = 258\n    # variables        = 12\n    chi-square         = 19116.9275\n    reduced chi-square = 77.7110875\n    Akaike info crit   = 1134.78543\n    Bayesian info crit = 1177.42095\n[[Variables]]\n    p1_E:       29.2403254 +/- 0.15073127 (0.52%) (init = 29)\n    p1_F:       1.70000000 +/- 0.49171585 (28.92%) (init = 1)\n    p1_area:    144.418628 +/- 121.500507 (84.13%) (init = 100)\n    p1_m:       4.6305e-11 +/- 144.803932 (312717811858121.94%) (init = 80)\n    p1_2_E:     31.1503254 +/- 0.15073127 (0.48%) == 'p1_E + 1.91'\n    p1_2_F:     1.70000000 +/- 0.49171585 (28.92%) == 'p1_F'\n    p1_2_area:  108.313971 +/- 91.1253806 (84.13%) == 'p1_area * 0.75'\n    p1_2_m:     4.6305e-11 +/- 144.803933 (312717812165592.81%) == 'p1_m'\n    p2_E:       27.4686616 +/- 0.10211334 (0.37%) (init = 27)\n    p2_F:       1.70000000 +/- 0.18372363 (10.81%) (init = 1)\n    p2_area:    198.605670 +/- 54.8065956 (27.60%) (init = 100)\n    p2_m:       79.8522534 +/- 19.5128773 (24.44%) (init = 80)\n    p2_2_E:     29.3786616 +/- 0.10211334 (0.35%) == 'p2_E + 1.91'\n    p2_2_F:     1.70000000 +/- 0.18372363 (10.81%) == 'p2_F'\n    p2_2_area:  148.954252 +/- 41.1049469 (27.60%) == 'p2_area * 0.75'\n    p2_2_m:     79.8522534 +/- 19.5128773 (24.44%) == 'p2_m'\n    p3_E:       30.8017623 +/- 0.06542507 (0.21%) (init = 31)\n    p3_F:       1.70000000 +/- 0.08028575 (4.72%) (init = 1)\n    p3_area:    447.908568 +/- 64.1021382 (14.31%) (init = 100)\n    p3_m:       9.69254060 +/- 16.8943567 (174.30%) (init = 80)\n    p3_2_E:     32.7117623 +/- 0.06542507 (0.20%) == 'p3_E + 1.91'\n    p3_2_F:     1.70000000 +/- 0.08028575 (4.72%) == 'p3_F'\n    p3_2_area:  335.931426 +/- 48.0766036 (14.31%) == 'p3_area * 0.75'\n    p3_2_m:     9.69254060 +/- 16.8943567 (174.30%) == 'p3_m'\n[[Correlations]] (unreported correlations are < 0.100)\n    C(p1_area, p3_area) = -0.980\n    C(p1_area, p2_area) = -0.975\n    C(p3_E, p3_F)       = -0.949\n    C(p1_E, p3_F)       = -0.948\n    C(p1_F, p2_E)       = -0.942\n    C(p1_m, p2_area)    = -0.923\n    C(p2_area, p3_area) =  0.917\n    C(p1_E, p3_E)       =  0.915\n    C(p1_F, p3_E)       =  0.903\n    C(p1_m, p3_m)       = -0.886\n    C(p1_area, p1_m)    =  0.883\n    C(p2_E, p2_F)       =  0.861\n    C(p1_F, p3_area)    = -0.855\n    C(p1_F, p3_F)       = -0.855\n    C(p1_F, p2_F)       = -0.826\n    C(p1_m, p3_area)    = -0.802\n    C(p2_E, p3_area)    =  0.801\n    C(p1_F, p1_area)    =  0.794\n    C(p2_E, p3_E)       = -0.790\n    C(p1_area, p2_E)    = -0.770\n    C(p1_E, p1_F)       =  0.743\n    C(p2_E, p3_F)       =  0.731\n    C(p2_F, p3_area)    =  0.720\n    C(p2_E, p2_area)    =  0.714\n    C(p1_area, p2_F)    = -0.713\n    C(p2_area, p3_m)    =  0.704\n    C(p1_F, p2_area)    = -0.697\n    C(p2_F, p2_area)    =  0.684\n    C(p1_area, p3_m)    = -0.653\n    C(p3_F, p3_area)    =  0.635\n    C(p3_E, p3_area)    = -0.631\n    C(p2_F, p3_E)       = -0.628\n    C(p1_E, p2_E)       = -0.594\n    C(p3_area, p3_m)    =  0.585\n    C(p1_m, p2_m)       = -0.555\n    C(p2_F, p3_F)       =  0.546\n    C(p2_m, p3_m)       =  0.533\n    C(p1_area, p3_F)    = -0.525\n    C(p1_area, p3_E)    =  0.514\n    C(p1_E, p3_area)    = -0.491\n    C(p1_m, p2_F)       = -0.449\n    C(p1_m, p2_E)       = -0.433\n    C(p1_F, p1_m)       =  0.419\n    C(p2_area, p2_m)    =  0.391\n    C(p2_m, p3_E)       =  0.388\n    C(p2_area, p3_F)    =  0.386\n    C(p2_area, p3_E)    = -0.374\n    C(p1_E, p1_area)    =  0.365\n    C(p1_E, p3_m)       =  0.356\n    C(p1_E, p2_F)       = -0.350\n    C(p2_F, p2_m)       = -0.317\n    C(p2_m, p3_F)       = -0.278\n    C(p1_area, p2_m)    = -0.267\n    C(p1_E, p2_m)       =  0.263\n    C(p2_F, p3_m)       =  0.257\n    C(p1_F, p2_m)       =  0.225\n    C(p3_F, p3_m)       = -0.218\n    C(p3_E, p3_m)       =  0.218\n    C(p1_E, p2_area)    = -0.214\n    C(p2_E, p2_m)       = -0.199\n    C(p2_E, p3_m)       =  0.177\n    C(p2_m, p3_area)    =  0.157\n    C(p1_F, p3_m)       = -0.129\n    C(p1_m, p3_F)       = -0.104\n"]}],"source":["fit = model.fit(intensity_zeroed[ind_max:ind_min],pars,x=be_vals[ind_max:ind_min],method=\"Least_squares\")\n","\n","print(fit.fit_report())\n","\n","final = (fit.best_fit) + shirley_back_fin[ind_max:ind_min]\n","comps = fit.eval_components(x=be_vals[ind_max:ind_min])\n","\n","for i in peak_input.peak_id[:peak_input.num_peaks]:\n","    comps[i] = comps[i] + shirley_back_fin[ind_max:ind_min]\n"]},{"source":["## Step 4/4: Run this block to plot the fit. You may have to change the labels and such.\n","Note: add auto-labeling for the sub-peaks, maybe via an additional parameter? Also make the shift more intuitive and user-friendly. Also make colors more easily configurable."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"output_type":"display_data","data":{"text/plain":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13140cc759c44f06bc99779ebc4aa2bb"}},"metadata":{}}],"source":["if peak_input.shift is None:\n","    peak_input.shift = 0\n","\n","peak_input.shift = -5\n","\n","x_min_shift = x_min + peak_input.shift\n","x_max_shift = x_max + peak_input.shift\n","\n","be_vals_shift = be_vals + peak_input.shift\n","\n","fig = plt.figure(figsize=(7,5.5))\n","\n","ax = fig.add_subplot(111)\n","\n","\n","ax.plot(be_vals_shift, intensity_vals, color='grey')       ## Plot Region in question\n","ax.plot(be_vals_shift[ind_max:ind_min],shirley_back_fin[ind_max:ind_min])    ## Plot Shirley background\n","\n","ax.plot(be_vals_shift[ind_max:ind_min], final, linewidth=3)    ## Final Fit\n","\n","for i in range(peak_input.num_peaks):\n","    ax.plot(be_vals_shift[ind_max:ind_min],comps[peak_input.peak_id[i]], label=peak_input.peak_id[i], linewidth=3)\n","\n","ax.set_title(label[activ_reg],fontweight='bold',fontsize=20)\n","\n","ax.set_xlim(x_min_shift,x_max_shift)\n","\n","ax.invert_xaxis()\n","ax.set_xlabel('Binding Energy [eV]',fontweight='bold',fontsize=16)\n","ax.set_ylabel('Intensity',fontweight='bold',fontsize=16)\n","ax.tick_params(axis='both',which='major',labelsize=14)\n","fig.subplots_adjust(hspace = 0.4)\n","\n","plt.legend(['Data', 'Background', 'Fit'],fontsize=10,loc=\"upper left\")\n","\n","ax.xaxis.set_minor_locator(MultipleLocator(100))\n","\n","def mouse_move(event):\n","    x, y = event.xdata, event.ydata\n","    print(x, y)\n","\n","plt.connect('motion_notify_event', mouse_move)\n","\n","plt.show()"]},{"source":["You're done! I haven't implemented a way to export the data yet, so just copy the plot and the results into powerpoint or something.\n","\n","Please email me at h.mou@columbia.edu or on Slack with any questions or bugs to report.\n","\n","Known issues:\n","- using product_gl method doesn't provide accurate areas\n","- no way to easily export the data"],"cell_type":"markdown","metadata":{}},{"source":["Right now I have to zero & normalize the raw data, because the fit equations max out at 1 and start at 0 intensity, before fitting. Then I undo everything to move the fitted plot onto the original raw data."],"cell_type":"markdown","metadata":{}}]}