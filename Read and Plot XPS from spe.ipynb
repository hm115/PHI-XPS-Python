{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Updated-tgmeow_Read and Plot Spectrum Data.ipynb","provenance":[{"file_id":"1-S9JK9aqv0B8-4G2iEovSZROuiCqWdU8","timestamp":1616527773421},{"file_id":"1RsPw2ZW0Z3LYVccJtxjC3FfbDIODYQ40","timestamp":1616526980863}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8"},"interpreter":{"hash":"78b9131e66c8710abe9bc5fbb97ea7fa8c5436852e45c0397a64dbdc6f6c3a21"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jWsQzLwbwOFS","executionInfo":{"status":"ok","timestamp":1623175921351,"user_tz":240,"elapsed":205,"user":{"displayName":"Hansen Mou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpX0kAdNL6lFvVSEqTwcLfrTvr1l-z768ZLhGZ=s64","userId":"05725151348395746586"}},"outputId":"2cbf1fee-7f09-49ae-c3be-de4e7306c78c"},"source":["# from google.colab import drive\r\n","\r\n","# drive.mount('/content/drive')"],"execution_count":3,"outputs":[]},{"source":["# Run the following 2 blocks. \n","## Then, start where it says Step 1/7. Run all blocks unless specified otherwise."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"5UECFNr8mttl","executionInfo":{"status":"ok","timestamp":1623175922772,"user_tz":240,"elapsed":206,"user":{"displayName":"Hansen Mou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpX0kAdNL6lFvVSEqTwcLfrTvr1l-z768ZLhGZ=s64","userId":"05725151348395746586"}}},"source":["import numpy as np\n","import struct\n","import array\n","\n","import ipywidgets as widgets\n","import matplotlib.pyplot as plt\n","import sys\n","import re\n","from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n","\n","from lmfit import Model\n","\n","import sys\n","import tkinter\n","from tkinter.filedialog import askopenfilenames"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M2ztiBIu4hTH"},"source":["The code block below is modified by Tiger Mou from the Python3convert-multipak-to-XPSPEAK-columbia.py file that was originally written by Robert Forest in Python 2, modified for Python 3 by Hansen Mou. Plotting and fitting was written by Hansen Mou."]},{"cell_type":"code","metadata":{"id":"OSSWTZLalUVz","executionInfo":{"status":"ok","timestamp":1623175925642,"user_tz":240,"elapsed":416,"user":{"displayName":"Hansen Mou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpX0kAdNL6lFvVSEqTwcLfrTvr1l-z768ZLhGZ=s64","userId":"05725151348395746586"}}},"source":["### This block imports the spe file.\n","\n","class XSpectrum:\n","\n","    def __init__(self):\n","        self._regions = []\n","        self._XPS_REGION_START_STR = b'DP'\n","        self._XPS_HEADER_SIZE = 42\n","   \n","    def load_xps(self, path):\n","        \"\"\"\n","        Read and parse XPS file according to the reverse engineering of\n","        Robert Forest's Python3convert-multipak-to-XPSPEAK-columbia.py\n","        \"\"\"\n","        file_str = \"\"\n","        # We read in the entire file because iterating through is tedious.\n","        with open(path, 'rb') as f:\n","            print('Reading XPS from ' + path)\n","            file_str = f.read();\n","            f.close()\n","\n","        version_str = file_str[0:11].decode('utf-8')\n","        print(f'File version is: {version_str}')\n","\n","        idx = file_str.find(self._XPS_REGION_START_STR) + len(self._XPS_REGION_START_STR)\n","        while idx != -1:\n","            print(f'IDX: {idx}')\n","            (region, size) = self.__xps_read_region(file_str, idx)\n","            self._regions.append(region)\n","            idx = file_str.find(self._XPS_REGION_START_STR, idx + size) + len(self._XPS_REGION_START_STR)\n","\n","        print('Done!')\n","    \n","  \n","    def load_spe(self, path):\n","        \"\"\"\n","        Read and parse SPE file according to the reverse engineering of\n","        Robert Forest's Python3convert-multipak-to-XPSPEAK-columbia.py\n","        \"\"\"\n","        with open(path, 'rb') as f:\n","            print('Reading SPE from ' + path)\n","            num_regions = self.__spe_get_number_of_regions(f)\n","            BE_parameters = self.__spe_get_BE_parameters(f, num_regions)\n","            self.__spe_convert_multipak_file(f, BE_parameters, num_regions)\n","        print('Done!')\n","\n","    # XPS helper functions\n","    def __xps_read_region(self, file_str, idx):\n","        \"\"\"\n","        \"\"\"\n","        curr_reg = dict()\n","\n","        \"\"\" Header contains: (total 42 bytes)\n","        0: region_size (2 bytes)\n","        1: unknown: uint 655353 (4)\n","        2: 20 characters for the region_name, with whitespace padding (20)\n","        3: region_size again ?? (2)\n","        4: unknown: int of 1 (2)\n","        5: region_size + 1 (2)\n","        and 10 zeros to end the header (10)\n","        \"\"\"\n","        region_header = struct.unpack_from('<HI20sHHH10x', file_str, idx)\n","        print(region_header)\n","        idx += self._XPS_HEADER_SIZE\n","\n","        curr_reg['region_size'] = region_header[0]\n","        curr_reg['region_name'] = region_header[2].strip()\n","        \n","        # Read region be\n","        curr_reg['be_data'] = np.array(struct.unpack_from(f'<{region_header[0]}f', file_str, idx))\n","        idx += region_header[0] * 4\n","\n","        # Strange whitespace of 1, region_size+1, and 10 zeros\n","        unknown_whitespace = struct.unpack_from('<HH10x', file_str, idx)\n","        idx += 14\n","\n","        # Read region intensity\n","        curr_reg['intensity'] = np.array(struct.unpack_from(f'<{region_header[0]}f', file_str, idx))\n","        idx += region_header[0] * 4\n","\n","        \"\"\" Footer contains:\n","        0: max_BE\n","        1: min_BE\n","        2: max_intensity\n","        3: min_intensity\n","        and LOTS of whitespace for some reason...\n","        \"\"\"\n","        region_footer = struct.unpack_from('<4f', file_str, idx)\n","        idx += 76\n","\n","        curr_reg['max_be'] = region_footer[0]\n","        curr_reg['min_be'] = region_footer[1]\n","        curr_reg['max_intensity'] = region_footer[2]\n","        curr_reg['min_intensity'] = region_footer[3]\n","\n","        # I think it's nothing but idk why\n","        region_post_footers = []\n","        for i in range(6):\n","            region_post_footers.append(struct.unpack_from(f'<HH10x{region_header[0]}f', file_str, idx + i*18+region_header[0]*4))\n","\n","        curr_reg['post_footers'] = region_post_footers\n","        \n","        region_size = self._XPS_HEADER_SIZE + region_header[0] * 4 * 2 + 14 + 16 + (6 *(2+2+region_header[0]*4+10)) + 18\n","\n","        return (curr_reg, region_size)\n","\n","\n","    # SPE helper functions\n","\n","    def __spe_get_number_of_regions(self, file):\n","        \"\"\"\n","        Returns the total number of elements (\"regions\") from multi-scan\n","        \"\"\"\n","        file_string = file.read()\n","        i = file_string.rfind(b'NoSpectralReg: ') + len('NoSpectralReg: ')\n","        file.seek(i)\n","        return int(file.readline())\n","\n","    def __spe_get_BE_parameters(self, file, num_regions):\n","        \"\"\"Returns a 2D list of binding energy parameters as strings\n","        First dimension is the region number\n","        Second dimension is a list of individual parameters for that region\n","        Indices: 3-region name, 5-region size, 6-step size, 7-high BE, 8-low BE\n","        \"\"\"\n","        BE_parameters = []\n","        for line_number in range(num_regions):\n","            BE_parameters.append(file.readline())\n","            BE_parameters[line_number] = BE_parameters[line_number].split(b' ')\n","        return BE_parameters\n","\n","    def __spe_convert_multipak_file(self, file, BE_parameters, num_regions):\n","        # Set up regions.\n","        self._regions = [dict() for _ in range(num_regions)]\n","\n","        # Go to start of data in multipak file.\n","        data_size = 0\n","        for i in range(num_regions):\n","            data_size += int(BE_parameters[i][5])\n","        file.seek(-8 * data_size, 2) \n","\n","        # Process data in each region.\n","        for i in range(num_regions):\n","            # Reference to regions dict.\n","            curr_reg = self._regions[i]\n","\n","            region_params = BE_parameters[i]\n","            curr_reg['max_be'] = float(region_params[7])\n","            curr_reg['min_be'] = float(region_params[8])\n","            curr_reg['region_size'] = int(region_params[5])\n","            intensity_data_double = array.array('d')\n","            intensity_data_double.fromfile(file, curr_reg['region_size'])\n","                        \n","            # Region metadata.  Not sure if useful.\n","            curr_reg['region_name'] = region_params[3]\n","\n","            # I guess XPS format doesn't support double?\n","            curr_reg['be_data'] = np.linspace(curr_reg['max_be'], curr_reg['min_be'], curr_reg['region_size'])\n","\n","            curr_reg['intensity'] = np.array(intensity_data_double)\n","\n","            curr_reg['min_intensity'] = min(curr_reg['intensity'])\n","            curr_reg['max_intensity'] = max(curr_reg['intensity'])\n","\n","def get_file_names():    \n","    root = tkinter.Tk()\n","    root.withdraw()\n","    root.attributes('-topmost',True)\n","    file_list = askopenfilenames(parent=root, title='Open XPS files',\n","                                              filetypes=[('.spe', '*.spe')])\n","    file_list = root.tk.splitlist(file_list) #workaround for Windows bug\n","    root.destroy()\n","    return file_list"],"execution_count":28,"outputs":[]},{"source":["## Step 1/7: Start here! Run the block below to import your SPE file and get a plot of the raw data.\n","You can mouse over the plot to see the coordinates!"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"2Q1WtsQzm6SM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623176511481,"user_tz":240,"elapsed":199,"user":{"displayName":"Hansen Mou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpX0kAdNL6lFvVSEqTwcLfrTvr1l-z768ZLhGZ=s64","userId":"05725151348395746586"}},"outputId":"21c776a3-ed0b-4d52-fdc4-7e70c2a1d3e8"},"source":["### Import file\n","file = get_file_names()\n","\n","data1 = XSpectrum()\n","\n","if len(file) > 0:\n","    data1.load_spe(file[0])\n","\n","### Identify the regions in the imported file\n","reg_len = len(data1._regions)\n","label = []\n","for i in range(reg_len):\n","    label.append(data1._regions[i]['region_name'].decode('UTF-8'))\n","print(\"\\nRegions identified: \" + str(label))\n","\n","### Assigning names to each set of data\n","id_data1 = {}\n","for i in range(reg_len):\n","    id_data1[label[i]] = data1._regions[i]\n","\n","\n","### Plot the raw data\n","%matplotlib widget\n","sub_tab=[widgets.Output() for i in range(reg_len)]\n","tab = widgets.Tab(sub_tab)\n","for i in range(reg_len):\n","    tab.set_title(i,label[i].format(i+1))\n","    with sub_tab[i]:\n","        fig = plt.figure(figsize=(10,5))\n","        ax = fig.subplots()\n","        ax.plot(data1._regions[i]['be_data'], data1._regions[i]['intensity'])\n","        ax.set_title(label[i],fontweight='bold',fontsize=20)\n","        ax.invert_xaxis()\n","        ax.set_xlabel('Binding Energy [eV]',fontweight='bold',fontsize=16)\n","        ax.set_ylabel('Intensity',fontweight='bold',fontsize=16)\n","        ax.tick_params(axis='both',which='major',labelsize=14)\n","        ax.xaxis.set_minor_locator(MultipleLocator(100))\n","        plt.show(fig)\n","display(tab)\n","def mouse_move(event):\n","    x, y = event.xdata, event.ydata\n","    print(x, y)\n","plt.connect('motion_notify_event', mouse_move)\n","plt.show()"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading SPE from C:/Users/hm115/Google Drive/Forschung/Data/XPS/XPS Data/NbC-film/210630-NbC-film-16'1A-15+58min2_fine0003.SPE\nDone!\n\nRegions identified: ['Nb3d', 'Pt4f7', 'C1s', 'O1s']\n"]},{"output_type":"display_data","data":{"text/plain":"Tab(children=(Output(), Output(), Output(), Output()), _titles={'0': 'Nb3d', '1': 'Pt4f7', '2': 'C1s', '3': 'O…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fba8301628234f17a5c92898d1956a99"}},"metadata":{}}]},{"source":["## Step 2/7: Choose the region you want to fit"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q63bMGS_MOtK","executionInfo":{"status":"ok","timestamp":1617823691188,"user_tz":240,"elapsed":30521,"user":{"displayName":"Hansen Mou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpX0kAdNL6lFvVSEqTwcLfrTvr1l-z768ZLhGZ=s64","userId":"05725151348395746586"}},"outputId":"26ef5fc9-1781-4004-e57b-9c934d7d13c9"},"source":["which_reg = \"Nb3d\"\n","\n","if which_reg in id_data1:\n","    be_vals = id_data1[which_reg]['be_data']\n","    activ_reg = label.index(which_reg)\n","    print(str(which_reg) + \" loaded!\")\n","else:\n","    print(\"The region you chose is not present. Please choose from the following:\")\n","    for i in id_data1:\n","        print(\"\\t\"+str(i))"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Nb3d loaded!\n"]}]},{"source":["## Step 3/7: Set the upper and lower bounds for the background"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Done!\n"]}],"source":["### Setting background\n","\n","background_lower_bound = 206.13\n","background_upper_bound = 216.03\n","\n","x_min = be_vals[np.abs(be_vals - background_lower_bound).argmin()]\n","x_max = be_vals[np.abs(be_vals - background_upper_bound).argmin()]\n","if x_min > x_max:\n","    print(\"Please switch your upper and lower bounds.\")\n","elif x_max not in be_vals or x_min not in be_vals:\n","    print(\"Your selected bounds exceed the region. Please set bounds between \" + str(be_vals[-1]) + \" and \" + str(be_vals[0]))\n","else:\n","    ind_min = np.where(be_vals == x_min)[0][0]\n","    ind_max = np.where(be_vals == x_max)[0][0]\n","\n","    intensity_vals = id_data1[which_reg]['intensity']\n","    y_min = id_data1[which_reg]['intensity'][ind_min]\n","    y_max = id_data1[which_reg]['intensity'][ind_max]\n","\n","    print(\"Done!\")"]},{"source":["### Implementing the Shirley Background Algorithm\n","(Should rewrite this into a function)"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NmvYCtH1aIcF","executionInfo":{"status":"ok","timestamp":1617823691193,"user_tz":240,"elapsed":30486,"user":{"displayName":"Hansen Mou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpX0kAdNL6lFvVSEqTwcLfrTvr1l-z768ZLhGZ=s64","userId":"05725151348395746586"}},"outputId":"409cb644-2180-41f4-ade5-4fb4544a99dc"},"source":["### Shirley background algorithm\n","\n","### Brief explanation of the algorithm: Shirley background is generated to be S(E) = y_min + interval * { A(2) / [A(1)+A(2)] }\n","##  Where S(E) is the background intensity at a given binding energy E\n","##  A(2) is the area at binding energies less than E, and A(1) is area at binding energies greater than E\n","##  interval is usually accepted to be the intensity difference of the lower and upper bounds for binding energy, for our background region of interest\n","##  This implementation first calculates the total area [A(1) + A(2)] by numerically integrating via the trapezoid method to find k_integral, and then k_val is simply \" interval / [A(1) + A(2)]\"\n","##  Next we calculate A(2) (designated y_integral) and multiply it with k_val\n","##  Once the iteration has completed, we add y_min to the background\n","##  Tolerance is determined by calculating the norm of the difference of the initial and final background arrays. I don't understand how that works, but norm basically indicates the degree of difference between 2 arrays, so the smaller the norm, the higher tolerance. Unit is arbitrary, I think.\n","\n","shirley_back = np.zeros(be_vals.shape)      ## initialize shirley_background array, same size as the entire domain of the region\n","for i in range(ind_min,ind_max+1):          ## populate shirley_back array with values for linear background\n","    shirley_back[i] = ((y_max - y_min)/(x_max - x_min)) * (be_vals[i] - x_min)\n","shirley_back_it = shirley_back.copy()       ## copy shirley_back array for a separate, iterated/manipulated version\n","\n","tol = 0.01      ## Tolerance level\n","\n","it_max = 1000       ## maximum number of iterations\n","it = 0              ## initial iteration value\n","\n","while it < it_max:\n","    k_integral = 0.0\n","    for i in range(ind_max, ind_min):\n","        k_integral += (be_vals[i+1] - be_vals[i]) * 0.5 * (intensity_vals[i+1] + intensity_vals[i] - 2 * y_min - shirley_back[i+1] - shirley_back[i]) # integration via trapezoid method\n","    k_val = (y_max - y_min)/k_integral\n","        \n","    for i in range(ind_max, ind_min):\n","        y_integral = 0.0\n","        for j in range(i, ind_min):\n","            y_integral += (be_vals[j+1] - be_vals[j]) * 0.5 * (intensity_vals[j+1] + intensity_vals[j] - 2 * y_min - shirley_back[j+1] - shirley_back[j])\n","        shirley_back_it[i] = k_val * y_integral\n","    \n","    if np.linalg.norm(shirley_back_it - shirley_back) < tol:\n","        shirley_back = shirley_back_it.copy()\n","        break\n","    else:\n","        shirley_back = shirley_back_it.copy()\n","    it +=1\n","\n","if it >= it_max:\n","    print(\"max iterations exceeded\")\n","\n","shirley_back_fin = y_min + shirley_back\n","\n","print(\"Number of iterations: \" + str(it))\n","# print(shirley_back_fin)\n","\n","### Replotting the region with the Shirley background\n","%matplotlib widget\n","fig = plt.figure(figsize=(10,5))\n","\n","i = activ_reg\n","\n","ax = fig.add_subplot(111)\n","ax.plot(be_vals, intensity_vals)       ## Plot Region in question\n","ax.plot(be_vals[ind_max:ind_min],shirley_back_fin[ind_max:ind_min])    ## Plot Shirley background\n","ax.set_title(label[i],fontweight='bold',fontsize=20)\n","ax.invert_xaxis()\n","plt.legend(['data', 'background'])\n","ax.set_xlabel('Binding Energy [eV]',fontweight='bold',fontsize=16)\n","ax.set_ylabel('Intensity',fontweight='bold',fontsize=16)\n","ax.tick_params(axis='both',which='major',labelsize=14)\n","fig.subplots_adjust(hspace = 0.4)\n","ax.xaxis.set_minor_locator(MultipleLocator(100))\n","def mouse_move(event):\n","    x, y = event.xdata, event.ydata\n","    print(x, y)\n","plt.connect('motion_notify_event', mouse_move)\n","plt.show()"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of iterations: 5\n"]},{"output_type":"display_data","data":{"text/plain":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7deb653184740c69f5931b41dddf6c1"}},"metadata":{}}]},{"source":["### Todo: incorporate peak area into fit model, fit paired dependent peaks, make process more user friendly"],"cell_type":"markdown","metadata":{}},{"source":["### Defining the basic fit equatons (Gaussian, Lorentzian)"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["### Gaussian\n","def gauss(x,E,F,area,m):\n","    # E = pars['E']\n","    # F = pars['F']\n","    # # a = param[2]\n","    # m = pars['m']\n","    # model = height* np.exp(-4 * np.log(2) * (1 - m/100) * ((x - E) / F)**2)\n","    model = (2 * np.sqrt(np.log(2))/(F * np.sqrt(np.pi))) * area * np.exp(-4 * np.log(2) * (1 - m/100) * ((x - E) / F)**2)\n","    return model\n","\n","# Note: for Gaussian, FWHM = 2 * sigma * sqrt(2 ln(2))\n","\n","### Lorentzian\n","def lorentz(x,E,F,area,m):\n","    # E = pars['E']\n","    # F = pars['F']\n","    # # a = param[2]\n","    # m = pars['m']\n","    # model = height / ((1 + 4 * m/100 * ((x - E) / F)**2))\n","    model = 2 * area / (np.pi * (1 + 4 * m/100 * ((x - E) / F)**2) * F)\n","    return model\n","# !!!! I don't understand why this equation shouldn't be multiplied by 2. !!!!\n","\n","# Note: for Lorentzian, FWHM = 2 sigma\n","\n","### E = position (eV); F = FWHM; m = % Lorentzian\n","\n","### Sum G/L\n","def sum_gl(x,E,F,area,m):\n","    # E = param[0]\n","    # F = param[1]\n","    # a = param[2]\n","    # m = param[3]\n","    model = ((1-m/100) * gauss(x,E,F,area,0) + (m/100) * lorentz(x,E,F,area,100))\n","    return model\n","\n","### Product G/L\n","## Has issues with returning the correct area. I think it has to do with how the weight factor 'm' is incorporated. May need to do numerical integration.\n","def product_gl(x,E,F,area,m):\n","    # E = pars['E']\n","    # F = pars['F']\n","    # # a = param[2]\n","    # m = pars['m']\n","    model = (gauss(x,E,F,area,m) * lorentz(x,E,F,area,m))\n","    return model"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["i = activ_reg\n","\n","intensity_zeroed = np.zeros(be_vals.shape)\n","\n","for j in range(len(be_vals)):\n","    intensity_zeroed[j] = intensity_vals[j] - shirley_back_fin[j]\n","\n","norm_factor = np.max(intensity_zeroed)\n","\n","intensity_norm = intensity_zeroed / norm_factor\n","#print(intensity_norm)\n","\n","# print(len(intensity_zeroed))\n","# print(norm_factor)\n","\n","# line_fit_zeroed = product_gl(data1._regions[i]['be_data'][ind_max:ind_min],80,param)\n","# line_fit_zeroed = product_gl(be_vals,param)\n","# line_fit_based = np.zeros(be_vals.shape)\n","\n","# for j in range(len(shirley_back_fin)):\n","#     line_fit_based[j] = line_fit_zeroed[j] + shirley_back_fin[j]"]},{"source":["## Step 4/7: Define your peaks and set initial Parameters\n","### Setting the Fit Parameters"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["### param = [peak binding energy, FWHM, area, % gauss/lorentz]\n","param1 = [210.5,1.0,300,80]\n","param2 = [213.7,1.0,200,80]\n","param3 = [208.8,1.0,100,80]\n","param4 = [211,1.0,80,80]"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"output_type":"display_data","data":{"text/plain":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4524ae0e111403fa07a23c4cb1ce78f"}},"metadata":{}}],"source":["### Plots the initial guess on the figure\n","\n","# This isn't automatically scaled yet. You need to add each one manually if you want to see the initial guess.\n","initial_fit_1 = sum_gl(be_vals[ind_max:ind_min],*param1) + shirley_back_fin[ind_max:ind_min]\n","initial_fit_2 = sum_gl(be_vals[ind_max:ind_min],*param2) + shirley_back_fin[ind_max:ind_min]\n","initial_fit_3 = sum_gl(be_vals[ind_max:ind_min],*param3) + shirley_back_fin[ind_max:ind_min]\n","initial_fit_4 = sum_gl(be_vals[ind_max:ind_min],*param4) + shirley_back_fin[ind_max:ind_min]\n","\n","# make sure that you multiply the shirley_back_fin by (total number of initial fits minus 1!)\n","initial_fit_sum = initial_fit_1 + initial_fit_2 + initial_fit_3 + initial_fit_4 - 3 * shirley_back_fin[ind_max:ind_min]\n","\n","fig = plt.figure(figsize=(10,5))\n","\n","ax = fig.add_subplot(111)\n","\n","\n","ax.plot(be_vals, intensity_vals)       ## Plot Region in question\n","ax.plot(be_vals[ind_max:ind_min],shirley_back_fin[ind_max:ind_min])    ## Plot Shirley background\n","ax.plot(be_vals[ind_max:ind_min],initial_fit_sum)        ## Initial Fit\n","ax.plot(be_vals[ind_max:ind_min],initial_fit_1)        ## Initial Fit\n","ax.plot(be_vals[ind_max:ind_min],initial_fit_2)        ## Initial Fit\n","\n","\n","\n","ax.set_title(label[activ_reg],fontweight='bold',fontsize=20)\n","ax.invert_xaxis()\n","ax.set_xlabel('Binding Energy [eV]',fontweight='bold',fontsize=16)\n","ax.set_ylabel('Intensity',fontweight='bold',fontsize=16)\n","ax.tick_params(axis='both',which='major',labelsize=14)\n","fig.subplots_adjust(hspace = 0.4)\n","\n","# plt.legend(['data', 'background', 'initial guess', 'final fit'])\n","plt.legend(['data', 'background', 'overall fit'])\n","\n","ax.xaxis.set_minor_locator(MultipleLocator(100))\n","\n","# ax[0].set_ylim(400,480)\n","# ax[1].set_ylim(0,500)\n","\n","def mouse_move(event):\n","    x, y = event.xdata, event.ydata\n","    print(x, y)\n","\n","plt.connect('motion_notify_event', mouse_move)\n","\n","plt.show()"]},{"source":["## Step 5.1/7: Run this block if fitting individual, unbound peaks. If not, do not run this block!!"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":453,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Parameters([('p1_E', <Parameter 'p1_E', value=534, bounds=[-inf:inf]>), ('p1_F', <Parameter 'p1_F', value=1.0, bounds=[0:1.7]>), ('p1_area', <Parameter 'p1_area', value=300, bounds=[0.1:inf]>), ('p1_m', <Parameter 'p1_m', value=80, bounds=[0:100]>), ('p2_E', <Parameter 'p2_E', value=536, bounds=[-inf:inf]>), ('p2_F', <Parameter 'p2_F', value=1.0, bounds=[0:1.7]>), ('p2_area', <Parameter 'p2_area', value=800, bounds=[0.1:inf]>), ('p2_m', <Parameter 'p2_m', value=80, bounds=[0:100]>)])\n"]}],"source":["peak_id = ['p1_','p2_']\n","\n","peak1 = Model(sum_gl,prefix=peak_id[0])\n","# peak1 = Model(product_gl,prefix=peak_id[0])\n","pars = peak1.make_params()\n","\n","pars['p1_E'].set(value=param1[0])\n","pars['p1_F'].set(value=param1[1],min=0,max=1.7)\n","pars['p1_area'].set(value=param1[2],min=0.1)\n","pars['p1_m'].set(value=param1[3],min=0,max=100)\n","\n","# pars['p1_m'].vary = False\n","\n","peak2 = Model(sum_gl,prefix=peak_id[1])\n","# peak2 = Model(product_gl,prefix=peak_id[1])\n","pars.update(peak2.make_params())\n","\n","pars['p2_E'].set(value=param2[0])\n","pars['p2_F'].set(value=param2[1],min=0,max=1.7)\n","pars['p2_area'].set(value=param2[2],min=0.1)\n","pars['p2_m'].set(value=param2[3],min=0,max=100)\n","\n","# pars['p2_m'].vary = False\n","\n","print(pars)\n","\n","model = peak1 + peak2"]},{"source":["## Step 5.2/7: Run this block if fitting couplets."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Parameters([('p1_E', <Parameter 'p1_E', value=210.5, bounds=[-inf:inf]>), ('p1_F', <Parameter 'p1_F', value=1.0, bounds=[0:1.7]>), ('p1_area', <Parameter 'p1_area', value=300, bounds=[0.1:inf]>), ('p1_m', <Parameter 'p1_m', value=80, bounds=[0:100]>), ('p1_2_E', <Parameter 'p1_2_E', value=213.22, bounds=[-inf:inf], expr='p1_E + 2.72'>), ('p1_2_F', <Parameter 'p1_2_F', value=1.0, bounds=[-inf:inf], expr='p1_F'>), ('p1_2_area', <Parameter 'p1_2_area', value=200.0, bounds=[-inf:inf], expr='p1_area * (2/3)'>), ('p1_2_m', <Parameter 'p1_2_m', value=80, bounds=[-inf:inf], expr='p1_m'>), ('p2_E', <Parameter 'p2_E', value=208.8, bounds=[-inf:inf]>), ('p2_F', <Parameter 'p2_F', value=1.0, bounds=[0:1.7]>), ('p2_area', <Parameter 'p2_area', value=100, bounds=[0.1:inf]>), ('p2_m', <Parameter 'p2_m', value=80, bounds=[0:100]>), ('p2_2_E', <Parameter 'p2_2_E', value=211.52, bounds=[-inf:inf], expr='p2_E + 2.72'>), ('p2_2_F', <Parameter 'p2_2_F', value=1.0, bounds=[-inf:inf], expr='p2_F'>), ('p2_2_area', <Parameter 'p2_2_area', value=66.66666666666666, bounds=[-inf:inf], expr='p2_area * (2/3)'>), ('p2_2_m', <Parameter 'p2_2_m', value=80, bounds=[-inf:inf], expr='p2_m'>)])\n"]}],"source":["### Main peak is 'p1_', bound couplet is 'p1_2'\n","\n","peak_id = ['p1_','p1_2_','p2_','p2_2_']\n","\n","peak1 = Model(sum_gl,prefix=peak_id[0])\n","# peak1 = Model(product_gl,prefix=peak_id[0])       ## product_gl doesn't provide accurate areas yet. Just use sum_gl\n","pars = peak1.make_params()\n","\n","# parameters pulled from the list param1\n","pars['p1_E'].set(value=param1[0])\n","pars['p1_F'].set(value=param1[1],min=0,max=1.7)\n","pars['p1_area'].set(value=param1[2],min=0.1)\n","pars['p1_m'].set(value=param1[3],min=0,max=100)\n","\n","peak2 = Model(sum_gl,prefix=peak_id[1])\n","# peak2 = Model(product_gl,prefix=peak_id[1])\n","pars.update(peak2.make_params())\n","\n","### Set how the parameters for peak1_2 are bound by peak 1\n","\n","# peak binding energy\n","pars['p1_2_E'].set(expr='p1_E + 2.72')\n","\n","# peak FWHM\n","pars['p1_2_F'].set(expr='p1_F')\n","\n","# peak area (could probably automate this by extracting the orbital from the region name)\n","pars['p1_2_area'].set(expr='p1_area * (2/3)')\n","\n","# % GL\n","pars['p1_2_m'].set(expr='p1_m')\n","##########################################\n","\n","### Couplet 2\n","peak3 = Model(sum_gl,prefix=peak_id[2])\n","# peak1 = Model(product_gl,prefix=peak_id[0])       ## product_gl doesn't provide accurate areas yet. Just use sum_gl\n","pars.update(peak3.make_params())\n","\n","# parameters pulled from the list param1\n","pars['p2_E'].set(value=param3[0])\n","pars['p2_F'].set(value=param3[1],min=0,max=1.7)\n","pars['p2_area'].set(value=param3[2],min=0.1)\n","pars['p2_m'].set(value=param3[3],min=0,max=100)\n","\n","\n","peak4 = Model(sum_gl,prefix=peak_id[3])\n","pars.update(peak4.make_params())\n","\n","### Set how the parameters for peak2_2 are bound by peak 2\n","\n","# peak binding energy\n","pars['p2_2_E'].set(expr='p2_E + 2.72')\n","\n","# peak FWHM\n","pars['p2_2_F'].set(expr='p2_F')\n","\n","# peak area (could probably automate this by extracting the orbital from the region name)\n","pars['p2_2_area'].set(expr='p2_area * (2/3)')\n","\n","# % GL\n","pars['p2_2_m'].set(expr='p2_m')\n","\n","print(pars)\n","\n","model = peak1 + peak2 + peak3 + peak4"]},{"source":["## Step 6/7: Run this block to perform the fit.\n","### Check the \"[[Variables]]\" block for the fitted parameters"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["[[Model]]\n    (((Model(sum_gl, prefix='p1_') + Model(sum_gl, prefix='p1_2_')) + Model(sum_gl, prefix='p2_')) + Model(sum_gl, prefix='p2_2_'))\n[[Fit Statistics]]\n    # fitting method   = least_squares\n    # function evals   = 13\n    # data points      = 198\n    # variables        = 8\n    chi-square         = 79464.1277\n    reduced chi-square = 418.232251\n    Akaike info crit   = 1202.96920\n    Bayesian info crit = 1229.27534\n[[Variables]]\n    p1_E:       211.670650 +/- 0.01096415 (0.01%) (init = 210.5)\n    p1_F:       1.47626972 +/- 0.03415589 (2.31%) (init = 1)\n    p1_area:    514.463173 +/- 19.8632172 (3.86%) (init = 300)\n    p1_m:       1.8663e-12 +/- 14.7429804 (789972774234380.25%) (init = 80)\n    p1_2_E:     214.390650 +/- 0.01096415 (0.01%) == 'p1_E + 2.72'\n    p1_2_F:     1.47626972 +/- 0.03415589 (2.31%) == 'p1_F'\n    p1_2_area:  342.975449 +/- 13.2421448 (3.86%) == 'p1_area * (2/3)'\n    p1_2_m:     1.8663e-12 +/- 14.7429804 (789972775513084.25%) == 'p1_m'\n    p2_E:       207.935837 +/- 0.03486819 (0.02%) (init = 208.8)\n    p2_F:       1.47156921 +/- 0.13557593 (9.21%) (init = 1)\n    p2_area:    256.130690 +/- 18.8653786 (7.37%) (init = 100)\n    p2_m:       86.8873271 +/- 23.1555408 (26.65%) (init = 80)\n    p2_2_E:     210.655837 +/- 0.03486819 (0.02%) == 'p2_E + 2.72'\n    p2_2_F:     1.47156921 +/- 0.13557593 (9.21%) == 'p2_F'\n    p2_2_area:  170.753794 +/- 12.5769190 (7.37%) == 'p2_area * (2/3)'\n    p2_2_m:     86.8873271 +/- 23.1555408 (26.65%) == 'p2_m'\n[[Correlations]] (unreported correlations are < 0.100)\n    C(p1_area, p1_m)    =  0.910\n    C(p1_area, p2_area) = -0.753\n    C(p2_area, p2_m)    =  0.736\n    C(p1_F, p1_m)       = -0.720\n    C(p2_F, p2_m)       = -0.699\n    C(p1_m, p2_area)    = -0.625\n    C(p1_F, p1_area)    = -0.486\n    C(p1_area, p2_m)    = -0.440\n    C(p1_F, p2_F)       =  0.352\n    C(p1_m, p2_m)       = -0.298\n    C(p1_F, p2_area)    =  0.235\n    C(p1_E, p2_m)       = -0.198\n    C(p2_F, p2_area)    = -0.171\n    C(p1_m, p2_E)       =  0.161\n    C(p1_m, p2_F)       = -0.140\n    C(p1_E, p2_E)       =  0.121\n"]}],"source":["fit = model.fit(intensity_zeroed[ind_max:ind_min],pars,x=be_vals[ind_max:ind_min],method=\"Least_squares\")\n","\n","print(fit.fit_report())\n","\n","final = (fit.best_fit) + shirley_back_fin[ind_max:ind_min]\n","comps = fit.eval_components(x=be_vals[ind_max:ind_min])\n","\n","for i in peak_id:\n","    comps[i] = comps[i] + shirley_back_fin[ind_max:ind_min]\n"]},{"source":["## Step 7/7: Run this block to plot the fit. You may have to change the labels and such."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"output_type":"display_data","data":{"text/plain":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eae50d82929343a4a5b08f05c645f56c"}},"metadata":{}}],"source":["fig = plt.figure(figsize=(10,5))\n","\n","ax = fig.add_subplot(111)\n","\n","\n","ax.plot(be_vals, intensity_vals)       ## Plot Region in question\n","ax.plot(be_vals[ind_max:ind_min],shirley_back_fin[ind_max:ind_min])    ## Plot Shirley background\n","\n","ax.plot(be_vals[ind_max:ind_min], final)    ## Final Fit\n","\n","ax.plot(be_vals[ind_max:ind_min],comps['p1_'], label='p1')\n","# ax.plot(be_vals[ind_max:ind_min],comps['p2_'], label='p2')        # left this behind for 2nd individual peak\n","ax.plot(be_vals[ind_max:ind_min],comps['p1_2_'], label='p1_2')      # left this behind for 1_2 couplet peak\n","ax.plot(be_vals[ind_max:ind_min],comps['p2_'], label='p2')\n","ax.plot(be_vals[ind_max:ind_min],comps['p2_2_'], label='p2_2')\n","\n","\n","ax.set_title(label[activ_reg],fontweight='bold',fontsize=20)\n","ax.invert_xaxis()\n","ax.set_xlabel('Binding Energy [eV]',fontweight='bold',fontsize=16)\n","ax.set_ylabel('Intensity',fontweight='bold',fontsize=16)\n","ax.tick_params(axis='both',which='major',labelsize=14)\n","fig.subplots_adjust(hspace = 0.4)\n","\n","plt.legend(['data', 'background', 'final fit'])\n","\n","ax.xaxis.set_minor_locator(MultipleLocator(100))\n","\n","def mouse_move(event):\n","    x, y = event.xdata, event.ydata\n","    print(x, y)\n","\n","plt.connect('motion_notify_event', mouse_move)\n","\n","plt.show()"]},{"source":["You're done! I haven't implemented a way to export the data yet, so just copy the plot and the results into powerpoint or something.\n","\n","Please email me at h.mou@columbia.edu or on Slack with any questions or bugs to report.\n","\n","Known issues:\n","- using product_gl method doesn't provide accurate areas\n","- current code doesn't easily scale to easily add more peaks\n","- not user friendly at all\n","- no way to easily export the data"],"cell_type":"markdown","metadata":{}},{"source":["Right now I have to zero & normalize the raw data, because the fit equations max out at 1 and start at 0 intensity, before fitting. Then I undo everything to move the fitted plot onto the original raw data."],"cell_type":"markdown","metadata":{}},{"source":["## Ignore the cell below for now. It is simply the normalized & zeroed version of the plot."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# %matplotlib widget\n","# fig = plt.figure(figsize=(10,5))\n","\n","# ax = fig.add_subplot(111)\n","\n","# ax.plot(be_vals, intensity_norm)       ## Plot Region in question\n","# ax.plot(be_vals[ind_max:ind_min],shirley_back[ind_max:ind_min]/norm_factor)    ## Plot Shirley background\n","# ax.plot(be_vals,line_fit_zeroed)\n","\n","# ax.set_title(label[i],fontweight='bold',fontsize=20)\n","# ax.invert_xaxis()\n","# ax.set_xlabel('Binding Energy [eV]',fontweight='bold',fontsize=16)\n","# ax.set_ylabel('Intensity',fontweight='bold',fontsize=16)\n","# ax.tick_params(axis='both',which='major',labelsize=14)\n","# fig.subplots_adjust(hspace = 0.4)\n","\n","# ax.xaxis.set_minor_locator(MultipleLocator(100))\n","\n","# # ax[0].set_ylim(400,480)\n","# # ax[1].set_ylim(0,500)\n","\n","# def mouse_move(event):\n","#     x, y = event.xdata, event.ydata\n","#     print(x, y)\n","\n","# plt.connect('motion_notify_event', mouse_move)\n","\n","# plt.show()"]}]}